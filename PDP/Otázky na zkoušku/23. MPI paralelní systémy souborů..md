- Někdy je potřeba číst, nebo zapisovat z velkých souborů
	- Například pro inicializaci dat procesů komunikátoru
- Můžeme se na to dívat tak, že čtení je jako příjem zprávy, zápis jako odesílání
- Pomocí klasických C/C++ funkcí není paralelní zápis do souboru více MPI Procesy najednou možný
	- Proto MPI standard definuje paralelní souborové operace - **MPI-I/O** funkce

# MPI-I/O
- Umožňuje současný přístup k různým částem souboru různým procesům
- Pro efektivní implementaci musí toto podporovat i filesystém
	- U klastrů se proto využívají paralelní souborové systémy (Lustre, GPFS)
	- Části souboru jsou mapovány přes I/O uzly (**OSS**) na úložná zařízení (**OST**)
		- Object Storage Server a Object Storage Target
- Kdy se to vyplatí?
	- Distribuovat soubor na více uzlů => jen pokud je opravdu velký
	- Velké množství malých souborů => použití více uzlů to může naopak zpomalit
- Počet OST je pro soubory implicitně nastaven na 1
	- Použití více uzlů musí uživatel nastavit na filesystému ručně

## Funkce a struktury
- Většina funkcí bere také `MPI_Comm` a otevírá proces ve všech procesech komunikátoru
- Soubor reprezentuje proměnná typu `MPI_File`
- Otevření a uzavření - `MPI_File_open`, `MPI_File_close`
- Nastavení pozice - `MPI_File_seek`
- Čtení, zápis - `MPI_File_read`, `MPI_File_write`
- Čtení / zápis z pozice - `MPI_File_read_at`, `MPI_File_write_at`

## Ošetření chyb
- Obdobně, jako u komunikátorů 
	- Error handler
	- Možnost nastavit vlastní pomocí `MPI_File_set_errhandler(MPI_Errhandler* errhandler)`
	- Implicitně je nastaven na `MPI_ERROR_RETURN` - je potřeba kontrolovat návratové kódy

## Příklad zápisu
- Máme paralelně běžící procesy, každý vygeneruje $m_i$ čísel (řádově $10^9$)
- Pro zápis musím vědět, kam se zapisuje - na to se použije exkluzivní PPS (prefixový součet na poli, kdy nezapočítávám svojí hodnotu) $$offset = \sum^{i-1}_{j=0}m_j $$
```c
int main(int argc, char* argv[]) 
{ 
	MPI_Init(&argc, &argv); 
	int proc_num; 
	MPI_Comm_rank(MPI_COMM_WORLD, &proc_num); 
	std::vector result; // výsledek výpočtu ... 
	// výpočet pole result 
	// výpočet pozice pro zápis dat jednotlivými procesy: 
	long my_size = result.size(); // velikost pole tohoto procesu 
	long my_offset; // offset v sdíleném souboru pro tento proces 
	// který se kolektivně vypočte exkluzivním PPS s operací scítání:
	MPI_Exscan(&my_size, &my_offset, 1, MPI_LONG, MPI_SUM, MPI_COMM_WORLD); 
	if (proc_num == 0) my_offset = 0; // hodnota procesu 0 není definována
	my_offset *= sizeof(double); 

	MPI_File f; //kolektivní otevření (případně vytvoření pokud neexistuje) 
	//souboru v režimu zápisu: 
	MPI_File_open(MPI_COMM_WORLD, "result.bin", //jméno v systému souborů
		MPI_MODE_CREATE | MPI_MODE_WRONLY, MPI_INFO_NULL, &f); //každý proces 
	// získá svůj vlastní file handler f 
	MPI_File_write_at(f, my_offset, &result[0], my_size, MPI_DOUBLE, MPI_STATUS_IGNORE); 
	MPI_File_close(&f); // kolektivní uzavření souboru 
	MPI_Finalize();
}
```

# Alternativy
- Práce s MPI-I/O je hodně nízkoúrovňová
- Existuje i jiná knihovna - HDF5, která obsahuje pohodlnější API
	- Dnes je to defakto standard pro práci s velkými soubory v HPC aplikacích
	- Zpřístupňuje uživateli abstraktní hierarchickou datovou strukturu HDF5 objektů
	- HDF5 objekt je skupina, nebo dataset
		- Skupina je množina linků na objekty
		- Dataset je homogenní pole, které se může za běhu měnit