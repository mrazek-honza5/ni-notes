# Paralelní škálovatelnost
- Schopnost paralelního algoritmu držet optimalitu, pokud $p$ a $n$ rostou nebo klesají
- Jinak
	- Je to schopnost paralelního počítače se zvětšit pokud narůstá velikost problému
	- Vyjadřuje, že větší problémy lze řešit za tentýž čas s více procesory
- Typy
	- **Silná škálovatelnost**
		- Měřítko rychlosti poklesu efektivnosti pokud $p$ roste a $n$ se nemění
	- **Slabá škálovatelnost**
		- Určuje měřítko růstu $n$ takového, že při rostoucím $p$ zůstává efektivnost stejná
# Amdahlův zákon
* Každý sekvenční algoritmus se proporčně skládá z
	* Inherentně sekvenčního podílu $f_s$, který může provést pouze jedno vlákno
	* Paralelizovatelného podílu $f_p=1-f_s$ 
* Když poté paralelizuju pomocí $p$ vláken, tak ideálně platí $$S(n,p)=\frac{T_A(n)}{f_s * T_A(n) + \frac{f_p}{p} * T_A(n)} = \frac{1}{f_s+\frac{f_p}{p}} \le \frac{1}{f_s}$$
* Zrychlení je tedy vždy saturováno velikostí proporce sekvenční a paralelizovatelné části
	* Například pokud mám $f_s = 10\%$,  tak při $p=\infty$ dostanu limitně $S(n,p)=\frac{1}{f_s+\frac{f_p}{p}} = \frac{1}{0,1+\frac{0,9}{\infty}} = \frac{1}{0,1+0}= 10$, tedy maximálně 10 krát zrychlení
* Amdahlův zákon tedy vyjadřuje maximální možný zisk z paralelizace
* Omezuje také silnou škálovatelnost algoritmu

# Gustafsonův zákon
* [[#Amdahlův zákon]] říká, že problém fixní velikosti poskytuje omezené množství paralelismu 
* Gustafsonův říká, že když zvětšujeme $p$, máme úměrně navyšovat velikost problému $n$
	* Jinými slovy: Gustafson říká, že když mám více procesorů, proč bych řešil stejně velký problém, když mohu ve stejném čase vyřešit větší problém?
	* Snaží se tím trochu řešit nedostatky Amdahlova zákona
* To vede na to, že $f_s$ trvá stále konstantní čas, ale $f_p$ bude v čase lineárně škálovat s $p$
* Potom $$S(n,p)=\frac{t_s+t_p(n,1)}{t_s+t_p(n,p)}$$
* Když předpokládáme, že paralelní část je perfektně paralelizovatelná, tak $$S(n,p)=\frac{t_s + SU(n) - t_s}{t_s + \frac{SU(n)-t_s}{p}}=\frac{\frac{t_s}{SU(n)-t_s}+1}{\frac{t_s}{SU(n)-t_s}+\frac{1}{p}}$$ a potom $\lim_{n->\infty} S(n,p)=p$   

# Izoefektivní funkce
> Je-li dána konstanta $0 < E_0 < 1$, pak izoefektivní funkce
> 	- $\psi_1(p)$ je asymptoticky **minimální** funkce taková, že vrací jaké nejmenší musí být $n$, aby byla konstantní efektivnost $E_0$ ($\forall n_p = \Omega(\psi_1(p)): E(n_p,p) \ge E_0$)
> 	- $\psi_2(n)$ je asymptoticky **minimální** funkce taková, že vrací nejvyšší počet procesorů, který můžeme použít, abychom měli konstantní efektivnost $E_0$ ($\forall p_n = O(\psi_2(n)): E(n, p_n) \ge E_0$ )

- Amdahlův zákon je jednoduchý argument, proč $p=\omega(\psi_2(n))$ saturuje paralelismus problému pro pevné $n$
- Gustafsonův zákon je jednoduchý argument, že když velikost problému roste s $p$ vztahem $n=\Omega(\psi_1(p))$, efektivnost nebude klesat